# Step2 Day2: 研修ワーク演習問題

## 概要

本演習では、Day1で学んだデータマネジメントの知識を基に、実務に即したデータ基盤の設計・構築を擬似体験します。

**グループ分け:**

それぞれが取り組むKPIや業務性質が異なるため、以下の２グループに分かれ、それぞれワークをすすめてください。

- 物流系部署グループ：
- 間接材購買系部署グループ：

**使用ツール:** 生成AI（GitHub Copilot、MS365 Copilot 等）を積極的に活用

**重要:** 本研修は非エンジニア向けです。コーディングやシステム設計は、**生成AIを活用して進めてください**。技術的な知識がなくても、適切なプロンプトを書くことで、生成AIが具体的なコードや設計を提案してくれます。

---

## 午前セッション

### ワーク①: KPI設定

#### 目的
ROIC（投下資本利益率）に関連するKPIを設定した上で、必要なデータ基盤について考える。

#### 演習内容

**Step 1: 「モニタリング・分析」対象のKPIを洗い出し、算出方法を設計する**

今回の演習用ダミーデータをよく確認した上で、ROICの構成要素（NOPAT、投下資本）を意識しながら、グループでモニタリングおよび分析できるKPIを3~5個洗い出してください。
また、洗い出したKPIごとに、KPIの算出方法（計算式）、KPIモニタリングのデータ項目設計、KPI改善分析に必要な分析軸なども定義してください。

**物流系の例:**
- 配送遅延率（OTD: On-Time Delivery）
- 在庫回転率
- 輸送コスト率（売上高対比）
- 緊急配送発生率

**間接材購買系（MRO）の例:**
- 設備稼働率（OEE: 設備総合効率）
- MRO在庫日数
- 緊急発注率（スポット購買率）
- ダウンタイム時間
- 設備故障による機会損失額

**生成AIの活用方法:**
```
＝＝＝プロンプト例＝＝＝
[演習用のデータや定義書をアップロード]
「自動車製造業の[物流部門/間接材購買部門]において、添付のデータや定義書を確認した上でROIC（投下資本利益率）の改善に直結するKPIを5つ提案してください。

各KPIについて以下を含めてください：
- KPI名
- 算出方法
- 必要なデータソース
- ROICへの貢献（NOPAT向上 or 投下資本削減）
- KPIモニタリングの代表的な項目
- KPI改善分析に利用できる分析軸

```

**提出物:** KPI一覧表（KPI名、計算式、目標値、データソース）


---

### ワーク②: データ基盤設計

#### 目的
ワーク①で設定したKPIを実現するために必要なデータ基盤を設計します。
グループで2~3つのKPIを選び、KPIごとに担当者（もしくは担当ペア）を決めてください。

#### 演習内容

**Step 2-1: 必要なデータソースを特定する**

KPIを算出するために必要なデータソースを洗い出してください。

**データソースの例:**
- ERP（財務、原価センタ）
- MES（製造実行システム：設備稼働、故障履歴）
- WMS（倉庫管理システム：在庫レベル、入出庫）
- TMS（輸配送管理システム：配送実績、リードタイム）
- P2P（購買管理システム：発注、請求履歴）
- others （社内システム：設備管理、カレンダー定義）

**生成AIの活用方法:**
```
＝＝＝プロンプト例＝＝＝
「以下のKPIを算出するために必要なデータソースを特定してください：
[ワーク①で設定したKPI一覧をコピー&ペースト]
[演習用のデータや定義書をアップロード]

各データソースについて以下を含めてください：
- 必要なデータの内容（例：発注履歴、在庫レベル）
- データの更新頻度（リアルタイム、日次、月次等）
- データの重要度（高/中/低）
表形式で出力してください。」
```

**提出物:** データソース一覧表（システム名、データ内容、更新頻度）

---

**Step 2-2: データマートのテーブル設計**

KPIを算出し、そのKPI用のモニタリング用テーブル（例えば、日報用エクセルシートみたいなもの。研修ではGoldデータとして紹介。データマートなどとも呼ばれる）を設計してください。

**設計すべきテーブルの例:**
1. **ファクトテーブル（トランザクションデータ）**
   - 物流系: 配送実績ファクト、在庫移動ファクト
   - 購買系: 請求ファクト、発注ファクト
   
2. **ディメンションテーブル（マスターデータ）**
   - 共通: 日付、拠点/倉庫、サプライヤー
   - 物流系: 輸送手段、ルート、顧客
   - 購買系: 品目、原価センタ、設備

**各テーブルに対して以下を定義してください:**
- テーブル名
- 主要なカラム（5~10個程度）
- 各カラムのデータ型
- 主キー（PK）と外部キー（FK）の関係

**ヒント:**
- データ粒度を明確に：1レコード（行）が何を表すか

**生成AIの活用方法:**
```
＝＝＝プロンプト例＝＝＝
自動車製造業の[物流/間接材購買]のデータ基盤のメダリオンアーキテクチャにおけるSilver/Goldのデータテーブルの設計書を作成してください。
Gold用のテーブルは、以下のKPIをモニタリングするために活用します。
Silver用のテーブルは、Gold用テーブルの元データとしても利用され、KPI算出の前処理やKPIの改善分析にも使われます。
なお、Bronze（生データ）は添付のデータソースです。
[自分が担当するKPIとその定義をコピー&ペースト/参照]
[必要なデータをアップロード/参照]

以下を含めてください：
 - Gold / Silver のテーブル定義書
 - 各テーブルを作成する方法（SQL等）
```

---

### ワーク③: データ基盤構築

#### 目的
設計したデータ基盤を実際に構築し、Bronze/Silver/Goldデータベースを作成し、分析テストを行う。

#### 演習内容

これまで設計したKPIのモニタリング/分析環境を実際に構築していきます。

 - 1. 必要なデータを演習用ダミーデータから選出。この時、初期構築(`initial_build/`)用csvファイルを利用してください。このcsvデータが今回のBronzeデータ（生データ）となります。
 - 2. 生成AIを活用して、BronzeからSilverに変換するプログラムを生成させ、Silverをデータベースに格納します。今回は、SQLite（軽量データベースエンジン）を使用します。
 - 3. 生成AIを活用して、SilverからGoldに変換するプログラムを生成させ、Goldテーブルをデータベースに格納します。
 - 4. 作成したGoldテーブルを用いて、生成AIにKPIのモニタリングを指示し、結果を確認します。

**生成AIの活用方法:**
```
＝＝＝プロンプト例＝＝＝
非エンジニアがローカル環境でデータ分析基盤を構築します。

# データ分析基盤の技術的な要件
- 元データ（Bronzeデータ）は、csvファイルです。
- Silver, GoldデータはSQLiteデータベースに格納します。
- SilverはKPI算出の前処理やKPI改善分析に利用されます。
- GoldはKPIモニタリングに利用されます。

# 業務要件
[前のワークで設定したKPIとその定義をコピー&ペースト/参照]
[演習用の必要なcsvデータとテーブル定義をアップロード/参照]
[Silver/Goldテーブル定義をアップロード/参照]
```

---

### ワーク④: データ活用（60分）

#### 目的
構築したデータ基盤を用いて、設定したKPIのモニタリングや分析を実行する。

#### 演習内容
前のワークで構築したデータベース（データ基盤）を模日イて、KPIのモニタリングと分析を実行してください。

 - 1. 組織が定時確認すべきKPIモニタリングレポートを複数パターン作成し、現状分析を行ってください。
 - 2. 担当するKPIについて深掘り分析を実行し、改善提案をまとめてください。
 - 3. 上記の1,2の結果をグループで共有し、ディスカッションしてください。

**生成AIの活用方法:**
```
＝＝＝KPIモニタリングレポートのプロンプト例＝＝＝
<構築したデータベースファイルを指定> を分析して、KPIモニタリングレポートを作成したいです。KPIを取り巻く現状を把握するために、複数の視点からレポートを作成してください。

# 例：
 - KPIの時系列推移
 - セグメント（拠点、サプライヤー等）別のKPI比較

[前のワークで設定したKPIとその定義をコピー&ペースト/参照]
[前のワークで作成したGoldテーブル定義をアップロード/参照]
```

```
＝＝＝KPI改善分析レポートのプロンプト例＝＝＝
<構築したデータベースファイルを指定> を分析して、KPIの改善分析レポートを作成したいです。私と対話的に分析をすすめ、KPI改善に向けた具体的な施策提案をまとめてください。

[前のワークで設定したKPIとその定義をコピー&ペースト/参照]
[前のワークで作成したGold/Silverテーブル定義をアップロード/参照]
[前のワークで作成したKPIモニタリングレポートをアップロード/参照]
```
---

### ワーク⑤: データ基盤の更新・改善（60分）

#### 目的
追加データを既存のデータ基盤に反映させ、基盤の運用・改善プロセスを体験する。

#### 演習内容

**Step 5-1: 更新データの投入（20分）**

`{各システム名}/update_data/` 配下にある「更新データセット」を、処理し、既存のデータ基盤に追加し、モニタリングレポートの更新テストを行ってください。

---

## よくある質問（FAQ）

**Q1: 生成AIはどこまで使ってよいですか？**
A: 積極的に活用してください。本研修の目的は「生成AIを活用してデータ基盤を構築できるスキル」を身につけることです。ただし、生成されたコードやクエリの実行結果に**必ず自分が責任を負う**ことを意識してください。コードを理解することが難しくても、コードをAIに解説させることは可能です。なんらかの方法で、実行内容を理解した上で使用してください。

**Q2: 生成AIの回答が間違っている場合はどうすればよいですか？**
A: 生成AIは完璧ではありません。以下の対処法を試してください：
- エラーメッセージを生成AIに貼り付けて、修正を依頼する
- AIに「もっと詳しく説明して」「レビューして」「作成したプログラムの動作検証テストをして」となどの追加指示を出す
- 講師に質問する

**Q3: 実務で使っているツール（例: AWS、Snowflake）を使ってもよいですか？**
A: 可能であれば問題ありません。ただし、本演習の範囲では、ローカル環境（SQLite等）で十分です。実務環境を使う場合は、**本番データを誤って触らないよう**十分注意してください。

**Q4: 生成AIへのプロンプトの書き方がわかりません。**
A: 各ワークに「プロンプト例」を記載していますので、それをコピー&ペーストして使ってください。慣れてきたら、自分の言葉でアレンジしてみましょう。

---

**Good Luck! 楽しみながら学びましょう！**
